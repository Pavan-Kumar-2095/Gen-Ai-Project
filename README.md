# ğŸš€ DocuQuery

**DocuQuery** is an open-source, LLM-powered Document Q&A API built with **FastAPI**, **FAISS**, and **Gemini 1.5 Flash**. It allows users to upload documents (PDF, DOCX, or EML) via URL and ask custom questions. The system returns concise, accurate answers generated by an LLM â€” using only the content of the uploaded document.

---

## ğŸ” Why Vector Embeddings?

Large Language Models (LLMs) are powerful, but processing entire documents directly can be inefficient:

- ğŸš« Excessive token usage
- ğŸ¢ Slower response times
- ğŸ’¸ Higher costs
- ğŸ¯ Less focused answers

**DocuQuery** solves this with **semantic search** and **context-aware retrieval**, ensuring answers are relevant and grounded in your data.

---

## ğŸ§  How It Works

1. ğŸ“¥ **Upload Document** (PDF, DOCX, EML via URL)
2. ğŸ“š **Parse & Chunk** document text
3. ğŸ§  **Generate Embeddings** with `all-MiniLM-L6-v2`
4. ğŸ” **Index Chunks** using FAISS
5. ğŸ¯ **Retrieve Relevant Chunks** for each question
6. âœ¨ **Generate Answer** with Gemini 1.5 Flash

---

## ğŸ› ï¸ Core Stack

| Feature | Tech |
|--------|------|
| **API Framework** | FastAPI |
| **Document Parsing** | PyMuPDF (PDF), Mammoth (DOCX), Python's `email` (EML) |
| **Chunking** | LangChainâ€™s `RecursiveCharacterTextSplitter` |
| **Embeddings** | SentenceTransformers (`all-MiniLM-L6-v2`) |
| **Vector Search** | FAISS (L2-normalized) |
| **LLM** | Gemini 1.5 Flash |

---

## âœ… Key Benefits

- ğŸ¯ **Highly Relevant Answers** using only document content
- âš¡ **Low Latency** and efficient token usage
- ğŸ’¡ **Improved LLM Performance** with focused context

---

## ğŸ”— Use Cases

- LLM-powered **Document Q&A**
- Internal **Knowledge Base Search**
- **RAG Pipelines** for enterprise automation
- AI-first **Document Workflows**

---

## ğŸš€ Getting Started

### ğŸ“¦ Installation

```bash
git clone https://github.com/Pavan-Kumar-2095/Gen-Ai-Project.git
cd Gen-Ai-Project
pip install -r requirements.txt

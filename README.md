# 🚀 DocuQuery

**DocuQuery** is an open-source, LLM-powered Document Q&A API built with **FastAPI**, **FAISS**, and **Gemini 1.5 Flash**. It allows users to upload documents (PDF, DOCX, or EML) via URL and ask custom questions. The system returns concise, accurate answers generated by an LLM — using only the content of the uploaded document.

---

## 🔍 Why Vector Embeddings?

Large Language Models (LLMs) are powerful, but processing entire documents directly can be inefficient:

- 🚫 Excessive token usage
- 🐢 Slower response times
- 💸 Higher costs
- 🎯 Less focused answers

**DocuQuery** solves this with **semantic search** and **context-aware retrieval**, ensuring answers are relevant and grounded in your data.

---

## 🧠 How It Works

1. 📥 **Upload Document** (PDF, DOCX, EML via URL)
2. 📚 **Parse & Chunk** document text
3. 🧠 **Generate Embeddings** with `all-MiniLM-L6-v2`
4. 🔍 **Index Chunks** using FAISS
5. 🎯 **Retrieve Relevant Chunks** for each question
6. ✨ **Generate Answer** with Gemini 1.5 Flash

---

## 🛠️ Core Stack

| Feature | Tech |
|--------|------|
| **API Framework** | FastAPI |
| **Document Parsing** | PyMuPDF (PDF), Mammoth (DOCX), Python's `email` (EML) |
| **Chunking** | LangChain’s `RecursiveCharacterTextSplitter` |
| **Embeddings** | SentenceTransformers (`all-MiniLM-L6-v2`) |
| **Vector Search** | FAISS (L2-normalized) |
| **LLM** | Gemini 1.5 Flash |

---

## ✅ Key Benefits

- 🎯 **Highly Relevant Answers** using only document content
- ⚡ **Low Latency** and efficient token usage
- 💡 **Improved LLM Performance** with focused context

---

## 🔗 Use Cases

- LLM-powered **Document Q&A**
- Internal **Knowledge Base Search**
- **RAG Pipelines** for enterprise automation
- AI-first **Document Workflows**

---

## 🚀 Getting Started

### 📦 Installation

```bash
git clone https://github.com/Pavan-Kumar-2095/Gen-Ai-Project.git
cd Gen-Ai-Project
pip install -r requirements.txt
